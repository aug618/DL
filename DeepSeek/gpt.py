import streamlit as st
from openai import OpenAI
import configparser
import PyPDF2
from docx import Document
import tiktoken

# è¯»å–é…ç½®æ–‡ä»¶
config = configparser.ConfigParser()
config.read('config.ini')

# è·å– API å¯†é’¥å’ŒåŸºç¡€ URL
api_key = config['openai']['api_key']
base_url = config['openai']['base_url']

# ä¾§è¾¹æ  UI é…ç½®
with st.sidebar:
    # è®¾ç½®æ ‡é¢˜å’Œå›¾åƒ
    st.markdown(f""" 
    <center>
    <img src="https://i.ibb.co/MgBss0Q/1.webp" alt="1" border="0" width="250" height="220">
    <h1>DeepSeek-V3<sup>ğŸ’¬</sup></h1>
    </center>""", unsafe_allow_html=True)
    
    # ç³»ç»Ÿæ¶ˆæ¯é…ç½®ï¼Œç”¨æˆ·å¯ä»¥è‡ªå®šä¹‰ç³»ç»Ÿæ¶ˆæ¯
    system_message = st.text_area("å®šä¹‰è§’è‰²", value="æˆ‘æ˜¯ç±³å¡”, ä½ å¯ä»¥é—®æˆ‘ä»»ä½•é—®é¢˜ğŸ¤£")
    
    # åˆ›é€ æ€§è°ƒèŠ‚å™¨ï¼Œè°ƒæ•´ç”Ÿæˆå†…å®¹çš„åˆ›æ„æ€§
    temperature = st.slider("Creativity", min_value=0.0, max_value=2.0, value=1.0, step=0.1,
                            help="The higher the value, the more creative the text will be.")

# ä¸»ç•Œé¢è®¾ç½®
st.title("ğŸ¦œç±³å¡”åŠ©æ‰‹")

# ä¸Šä¼ æ–‡ä»¶
uploaded_files = st.file_uploader("ä¸Šä¼ æ–‡ä»¶", type=["txt", "pdf", "docx"], accept_multiple_files=True)

# å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶
documents = []
for file in uploaded_files:
    if file.type == "text/plain":
        # å¤„ç†æ–‡æœ¬æ–‡ä»¶
        file_content = file.read().decode("utf-8")
        documents.append(file_content)
    elif file.type == "application/pdf":
        # å¤„ç† PDF æ–‡ä»¶
        pdf_reader = PyPDF2.PdfReader(file)
        text = ""
        for page in pdf_reader.pages:
            text += page.extract_text()
        documents.append(text)
    elif file.type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
        # å¤„ç† Word æ–‡ä»¶
        doc = Document(file)
        text = "\n".join([paragraph.text for paragraph in doc.paragraphs])
        documents.append(text)
    else:
        st.warning(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file.type}")

# ä½¿ç”¨ session_state åˆå§‹åŒ–èŠå¤©è®°å½•
if "chat_history" not in st.session_state:
    st.session_state.chat_history = [{
        "role": "assistant",
        "content": "æˆ‘æ˜¯ç±³å¡”, ä½ å¯ä»¥é—®æˆ‘ä»»ä½•é—®é¢˜ğŸ¤£"
    }]

# æ˜¾ç¤ºèŠå¤©è®°å½•
for message in st.session_state.chat_history:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# openai å®¢æˆ·ç«¯åˆå§‹åŒ–
client = OpenAI(api_key=api_key, base_url=base_url)
if "messageHistory" not in st.session_state:
    st.session_state.messageHistory = []

# å®šä¹‰å‡½æ•°è®¡ç®— token æ•°
def count_tokens(text, model="gpt-4"):
    encoding = tiktoken.encoding_for_model(model)
    return len(encoding.encode(text))

# å®šä¹‰å‡½æ•°æ¸…ç†èŠå¤©å†å²è®°å½•
def trim_chat_history(chat_history, max_tokens=50000):
    encoding = tiktoken.encoding_for_model("gpt-4")
    total_tokens = 0
    trimmed_history = []
    for message in reversed(chat_history):
        tokens = count_tokens(message["content"])
        if total_tokens + tokens > max_tokens:
            break
        trimmed_history.append(message)
        total_tokens += tokens
    return list(reversed(trimmed_history))

# å®šä¹‰èŠå¤©æµå¤„ç†å‡½æ•°
def chat_stream(query, system_message=None, temperature=0.5, context=None):
    if system_message:
        st.session_state.messageHistory.append({"role": "system", "content": system_message})
    
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²è®°å½•
    st.session_state.messageHistory.append({"role": "user", "content": query})

    # å°†ä¸Šä¼ çš„æ–‡ä»¶å†…å®¹ä½œä¸ºä¸Šä¸‹æ–‡ä¼ é€’ç»™æ¨¡å‹
    if context:
        st.session_state.messageHistory.append({"role": "system", "content": context})

    # æ¸…ç†èŠå¤©å†å²è®°å½•ï¼Œç¡®ä¿ä¸è¶…è¿‡æœ€å¤§ token æ•°
    st.session_state.messageHistory = trim_chat_history(st.session_state.messageHistory)

    # è°ƒç”¨ DeepSeek API è·å–èŠå¤©æµæ•°æ®
    response = client.chat.completions.create(
        model="deepseek-chat", 
        messages=st.session_state.messageHistory, 
        stream=True, 
        temperature=temperature
    )
    return response

# å¤„ç†ç”¨æˆ·è¾“å…¥å¹¶æ˜¾ç¤º
user_input = st.chat_input("ä½ æƒ³é—®æˆ‘ä»€ä¹ˆé—®é¢˜?", key="user_input")
if user_input:
    # æ˜¾ç¤ºç”¨æˆ·è¾“å…¥
    with st.chat_message("user"):
        st.write(user_input)

    # å°†ç”¨æˆ·è¾“å…¥æ·»åŠ åˆ°èŠå¤©å†å²è®°å½•
    st.session_state.chat_history.append({"role": "user", "content": user_input})

    # å‡†å¤‡ä¸Šä¸‹æ–‡
    context = "\n".join(documents) if documents else None

    # æ˜¾ç¤ºåŠ è½½çŠ¶æ€å¹¶è°ƒç”¨ DeepSeek æ¥å£
    with st.spinner("è®©æˆ‘æƒ³ä¸€æƒ³å…ˆ..."):
        response = chat_stream(user_input, system_message, temperature, context)
        message_placeholder = st.empty()
        full_response = ""

        # æŒ‰æµå¼æ•°æ®è·å–å†…å®¹å¹¶æ›´æ–°æ˜¾ç¤º
        for chunk in response:
            if chunk.choices and chunk.choices[0].delta.content:
                full_response += chunk.choices[0].delta.content
                message_placeholder.markdown(full_response + "...")

        # å®Œæˆæ‰€æœ‰å†…å®¹åæ˜¾ç¤ºæœ€ç»ˆå›ç­”
        message_placeholder.markdown(full_response)

        # å°† AI å“åº”æ·»åŠ åˆ°èŠå¤©å†å²è®°å½•
        st.session_state.chat_history.append({"role": "assistant", "content": full_response})